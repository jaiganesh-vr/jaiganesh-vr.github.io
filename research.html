<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Blog</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
    
<header>
  <nav>
    <a href="index.html">Blog</a>
    <a href="learn.html" class="active">Learn</a>
  </nav>
</header>

<main>
  <h1>Thoughts on building AI things </h1>
  <p class="muted"> Notes that sparked ideas, books that taught me new technologies, projects that turned theory into practice,
  and research that added depth - all in one place, documenting how I learn, experiment, and apply ideas in real systems.
  </p>

<menu class="menu-highlight">
  <a href="learn.html">Notes</a>
  <a href="books.html">Books</a>
  <a href="projects.html">Projects</a>
  <a href="research.html" aria-current="page">Research</a>
</menu>

  <div class="divider"></div>

  <p>Exploring groundbreaking research, Interpreting findings accurately,
  Reviewing existing literature and Drawing informed conclusions</p>

  <div class="divider"></div>

  <h2>Recursive Language Models</h2>
  <p class="muted">Lowering memory consumption and increasing training speed of BERT by introducing two parameter-reduction 
  techniques overcoming the GPU/TPU training limitations
  </p>

  <p class="repo">
    <a href="research/20260127_rlm.html">
      Read more
    </a>
  </p>

  <h2>Llama 3 Herd of Models</h2>
  <p class="muted">Set of advanced foundation models supporting multilinguality, coding, reasoning, 
  and tool use, with up to 405B parameters. It rivals GPT-4 in performance, integrates image, video, 
  and speech capabilities, and includes safety features via Llama Guard 3
  </p>

  <p class="repo">
    <a href="research/20260127_llama.html">
      Read more
    </a>
  </p>

  <h2>Attention Is All You Need!</h2>
  <p class="muted"> The Transformer network revolutionizes sequence transduction models by replacing 
  conventional recurrent and convolutional structures, setting new benchmarks in various 
  machine learning tasks
  </p>

  <p class="repo">
    <a href="research/20260127_attention.html">
      Read more
    </a>
  </p>

  <h2>BERT Pre-training of Deep Bidirectional Transformers for Language Understanding</h2>
  <p class="muted">Bidirectional Encoder Representations from Transformers designed to pre-train deep bidirectional 
  representations from unlabelled text by jointly conditioning on both left and right context in all layers
  </p>

  <p class="repo">
    <a href="https://github.com/jaiganesh-vr">
      Read more
    </a>
  </p>

  <h2>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h2>
  <p class="muted">Lowering memory consumption and increasing training speed of BERT by introducing two parameter-reduction 
  techniques overcoming the GPU/TPU training limitations
  </p>

  <p class="repo">
    <a href="https://github.com/jaiganesh-vr">
      Read more
    </a>
  </p>

  <div class="divider"></div>

  <footer class="site-footer">
      <a href="https://github.com/jaiganesh-vr" target="_blank">↗ Github</a>
      <a href="https://www.upwork.com/" target="_blank">↗ Upwork</a>
      <a href="https://www.linkedin.com/in/jaiganesh-vr/" target="_blank">↗ LinkedIn</a>
  </footer>
    
</main>
</body>
</html>
