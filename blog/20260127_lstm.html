<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Long Short-Term Memory Networks (LSTMs)</title>
  <link rel="stylesheet" href="../styles.css">
</head>
<body>

<header>
  <nav>
    <a href="../index.html">Blog</a>
    <a href="../learn.html">Learn</a>
  </nav>
</header>

<main>

  <h1>Long Short-Term Memory Networks (LSTMs)</h1>

  <p class="muted">
    <span class="pill">AI</span>
    <span class="pill">Deep Learning</span>
    <span class="pill">Neural Networks</span>
  </p>
  <p class="muted">
     Oct 10, 2025
  </p>

  <p>
    Neural networks are computational models inspired by the human brain, designed to
    recognize patterns and relationships in data. While traditional architectures perform
    well on fixed-size inputs, they struggle when information needs to be remembered over time.
  </p>

  <p>
    Tasks such as language modeling, speech recognition, and time-series prediction require
    models to capture long-range dependencies—something standard neural networks are not
    designed to handle effectively.
  </p>

  <h2>The Limitation of Traditional Neural Networks</h2>

  <p>
    Feedforward neural networks and basic recurrent neural networks (RNNs) have difficulty
    retaining information from earlier inputs when processing long sequences. As the sequence
    grows, important signals tend to fade or explode during training, making learning unstable
    and ineffective.
  </p>

  <p>
    This limitation makes them poorly suited for problems where context from the distant past
    matters.
  </p>

  <h2>What LSTMs Solve</h2>

  <p>
    Long Short-Term Memory (LSTM) networks were designed specifically to address the problem of
    long-term dependency. Unlike traditional networks, LSTMs introduce a cell state that allows
    information to persist over long periods of time.
  </p>

  <p>
    Instead of trying to remember everything, LSTMs learn what information to keep, update, or
    discard through a set of gated mechanisms.
  </p>

  <h2>Inside an LSTM Cell</h2>

  <p>
    Each LSTM cell contains three gates that regulate the flow of information: the forget gate,
    the input gate, and the output gate. These gates are implemented using sigmoid activation
    functions and element-wise multiplication.
  </p>

  <img
    src="../img/img_lstm1.jpg"
    alt="Healthy frozen waffles billboard"
    class="post-hero"
    />

  <p>
    The sigmoid function outputs values between 0 and 1, where 0 means no information passes
    through and 1 means full passage of information.
  </p>

  <img
    src="../img/img_lstm2.jpg"
    alt="Healthy frozen waffles billboard"
    class="post-hero"
  />

  <h2>Forget Gate</h2>

  <p>
    The forget gate determines which information from the previous cell state should be removed.
    It takes the previous hidden state and the current input as input and produces a value
    between 0 and 1 for each element of the cell state.
  </p>

  <p>
    A value close to 1 means the information is retained, while a value close to 0 means it is
    discarded.
  </p>

  <h2>Input Gate</h2>

  <p>
    The input gate controls what new information is added to the cell state. It consists of two
    components:
  </p>

  <ul class="post-list">
    <li>
      A sigmoid layer that decides which parts of the cell state should be updated.
    </li>
    <li>
      A tanh layer that generates candidate values, representing new information that could be
      added to the state.
    </li>
  </ul>

  <p>
    The cell state is updated by combining the retained old information with the new candidate
    values, scaled by how much influence they should have.
  </p>

  <h2>Output Gate</h2>

  <p>
    The output gate determines what information from the cell state is exposed as the hidden
    state. A sigmoid layer selects which parts of the cell state will be output.
  </p>

  <p>
    The cell state is passed through a tanh activation to constrain values between -1 and 1,
    and then multiplied by the output gate’s decision. This ensures that only relevant
    information is propagated forward.
  </p>

  <p>
    By explicitly controlling memory through gates, LSTMs enable neural networks to learn
    long-term dependencies more effectively. This makes them a foundational architecture for
    sequence-based tasks in deep learning.
  </p>

  <div class="divider"></div>

  <p class="muted">
    Notes like these help bridge theory and implementation—understanding how models work before
    deciding where they should be used.
  </p>

</main>
</body>
</html>
